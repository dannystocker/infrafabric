# SÂ² Multi-Machine Deployment Guide

**Architecture:** 9 agents on **9 separate machines** (no shared filesystem)
**Coordination:** Git repo + MCP Bridge
**Ready to Deploy:** Yes âœ…

---

## Quick Reference: What to Paste Where

### On ALL 9 Machines (Orchestrator + 8 Workers)

**Step 1: Install MCP Bridge**

```bash
# Paste this into each Claude session
curl -fsSL https://raw.githubusercontent.com/dannystocker/mcp-multiagent-bridge/main/scripts/install.sh 2>/dev/null | bash || { cd /tmp && git clone https://github.com/dannystocker/mcp-multiagent-bridge.git && cd mcp-multiagent-bridge && pip install -q mcp>=1.0.0 && mkdir -p ~/.config/claude && echo "{\"mcpServers\":{\"bridge\":{\"command\":\"python3\",\"args\":[\"$(pwd)/claude_bridge_secure.py\"]}}}" > ~/.config/claude/claude.json && echo "âœ… MCP Bridge installed at: $(pwd)" && echo "ðŸ“ Config: ~/.config/claude/claude.json" && echo "ðŸ”„ Restart Claude Code to load MCP server"; }
```

**Step 2: Pull Deployment Scripts**

```bash
cd /home/user/infrafabric
git pull origin claude/debug-session-freezing-011CV2mM1FVCwsC8GoBR2aQy
chmod +x scripts/s2-deployment/*.sh scripts/s2-deployment/*.py
```

**Expected output:**
```
âœ… MCP Bridge installed at: /tmp/mcp-multiagent-bridge
ðŸ“ Config: ~/.config/claude/claude.json
```

---

### On Orchestrator Machine ONLY (Cloud Machine 1 - This Session)

**Step 3: Create 8 Conversations**

Paste this into **this Claude session**:

```
I need to create 8 conversations for SÂ² coordination using the MCP bridge.

For each worker (1-8):
  1. Use the MCP tool: create_conversation
     - my_role: "s2-orchestrator"
     - partner_role: "worker-{1-8}-{role}"

  2. Save credentials to: credentials/s2-worker-{1-8}-credentials.json

     Format:
     {
       "conversation_id": "conv_...",
       "worker_id": "{1-8}",
       "worker_role": "{role}",
       "coordinator_token": "token_...",
       "worker_token": "token_...",
       "created_at": "timestamp",
       "expires_at": "timestamp"
     }

Worker roles:
- worker-1-backend
- worker-2-frontend
- worker-3-tests
- worker-4-docs
- worker-5-deploy
- worker-6-review
- worker-7-wsl-cli
- worker-8-cloud-codex

After creating all 8 conversations, commit credentials to git:

cd /home/user/infrafabric
git add credentials/s2-worker-*.json
git commit -m "chore(s2): Add MCP bridge credentials for 8 workers"
git push -u origin claude/debug-session-freezing-011CV2mM1FVCwsC8GoBR2aQy
```

**Step 4: Start External Watchdog**

After credentials are created:

```bash
cd /home/user/infrafabric
scripts/s2-deployment/watchdog-monitor.sh &
echo "ðŸ• Watchdog started - monitoring all 8 workers"
tail -f /tmp/mcp-watchdog.log
```

---

### On Worker Machines (8 Machines)

**Machine Assignments:**

| Machine | Worker ID | Role | Command to Paste |
|---------|-----------|------|------------------|
| Cloud Machine 2 | 1 | backend | `export WORKER_ID=1` |
| Cloud Machine 3 | 2 | frontend | `export WORKER_ID=2` |
| Cloud Machine 4 | 3 | tests | `export WORKER_ID=3` |
| Cloud Machine 5 | 4 | docs | `export WORKER_ID=4` |
| Cloud Machine 6 | 5 | deploy | `export WORKER_ID=5` |
| Cloud Machine 7 | 6 | review | `export WORKER_ID=6` |
| Your WSL (Local) | 7 | wsl-cli | `export WORKER_ID=7` |
| Cloud Machine 8 | 8 | cloud-codex | `export WORKER_ID=8` |

**Step 5: Pull Credentials and Start Keep-Alive (Each Worker)**

```bash
# Set worker ID for this machine (see table above)
export WORKER_ID=1  # Change to 1, 2, 3, 4, 5, 6, 7, or 8

# Pull latest credentials from git
cd /home/user/infrafabric
git pull origin claude/debug-session-freezing-011CV2mM1FVCwsC8GoBR2aQy

# Install jq if not already installed (needed to parse JSON)
sudo apt-get install -y jq

# Extract credentials
CONV_ID=$(jq -r '.conversation_id' credentials/s2-worker-${WORKER_ID}-credentials.json)
WORKER_TOKEN=$(jq -r '.worker_token' credentials/s2-worker-${WORKER_ID}-credentials.json)

# Start keep-alive daemon
scripts/s2-deployment/keepalive-daemon.sh "$CONV_ID" "$WORKER_TOKEN" &

echo "âœ… Worker-$WORKER_ID keep-alive daemon started"
echo "ðŸ“‹ Conversation ID: $CONV_ID"
echo "ðŸ“ Logs: tail -f /tmp/mcp-keepalive.log"
```

**Optional: Start Credential Sync Daemon**

If you want automatic credential updates when orchestrator creates new conversations:

```bash
scripts/s2-deployment/credential-sync-daemon.sh $WORKER_ID &
echo "ðŸ”„ Credential sync daemon started (auto-pulls every 60s)"
```

---

## Verification

### On Orchestrator (After All Workers Started)

Check that all 8 workers are sending heartbeats:

```bash
sqlite3 /tmp/claude_bridge_coordinator.db "SELECT conversation_id, last_heartbeat FROM session_status ORDER BY last_heartbeat DESC;"
```

**Expected output:**
```
conv_worker1|2025-11-13T16:30:15.123456
conv_worker2|2025-11-13T16:30:14.234567
conv_worker3|2025-11-13T16:30:13.345678
conv_worker4|2025-11-13T16:30:12.456789
conv_worker5|2025-11-13T16:30:11.567890
conv_worker6|2025-11-13T16:30:10.678901
conv_worker7|2025-11-13T16:30:09.789012
conv_worker8|2025-11-13T16:30:08.890123
```

### Send Test Task to All Workers

Paste into orchestrator Claude session:

```
Send a test message to all 8 workers:

For each worker (1-8):
  Use MCP tool: send_to_partner
  - conversation_id: [worker-{n}-conv-id]
  - message: {
      "type": "test_task",
      "task_id": "comms-test",
      "description": "Reply with 'ACK from worker-{n}'"
    }
  - action_type: "task_assignment"

Then monitor for responses:

Every 30 seconds for 2 minutes:
  Use MCP tool: check_messages
  Log any acknowledgments received
```

**Expected:** All 8 workers send "ACK from worker-X" within 60 seconds.

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Git Repository (Shared Coordination)           â”‚
â”‚         dannystocker/infrafabric                       â”‚
â”‚                                                        â”‚
â”‚  â€¢ scripts/s2-deployment/ (all machines pull)          â”‚
â”‚  â€¢ credentials/ (orchestrator creates, workers pull)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                       â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚  Cloud    â”‚  â”‚  Cloud      â”‚  â”‚ Cloud â”‚  â”‚ Cloud â”‚
â”‚ Machine 1 â”‚  â”‚ Machine 2   â”‚  â”‚ Mach 3â”‚  â”‚ Mach 4â”‚
â”‚           â”‚  â”‚             â”‚  â”‚       â”‚  â”‚       â”‚
â”‚Orchestratorâ”‚ â”‚ Worker-1    â”‚  â”‚Worker2â”‚  â”‚Worker3â”‚
â”‚ + Watchdogâ”‚  â”‚ (Backend)   â”‚  â”‚(Front)â”‚  â”‚(Tests)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                       â”‚           â”‚           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                       â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  Cloud    â”‚  â”‚  Cloud      â”‚  â”‚ Cloud â”‚  â”‚Your WSLâ”‚
â”‚ Machine 5 â”‚  â”‚ Machine 6   â”‚  â”‚ Mach 7â”‚  â”‚(Local) â”‚
â”‚           â”‚  â”‚             â”‚  â”‚       â”‚  â”‚        â”‚
â”‚ Worker-4  â”‚  â”‚ Worker-5    â”‚  â”‚Worker6â”‚  â”‚Worker7 â”‚
â”‚ (Docs)    â”‚  â”‚ (Deploy)    â”‚  â”‚(Review)â”‚ â”‚(CLI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Cloud    â”‚
                  â”‚Machine 8  â”‚
                  â”‚           â”‚
                  â”‚ Worker-8  â”‚
                  â”‚ (Codex)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**
- âœ… No shared `/tmp` or local directories
- âœ… All coordination via git commits
- âœ… Each machine pulls scripts and credentials independently
- âœ… Keep-alive daemons run locally on each worker
- âœ… External watchdog runs on orchestrator machine only

---

## Common Issues

### Issue: "Credentials file not found"

**Symptom:**
```
âš ï¸  Credentials file not found: /home/user/infrafabric/credentials/s2-worker-1-credentials.json
```

**Solution:**
```bash
# Make sure you pulled latest from git
cd /home/user/infrafabric
git pull origin claude/debug-session-freezing-011CV2mM1FVCwsC8GoBR2aQy

# Verify credentials exist
ls -lh credentials/s2-worker-*.json
```

### Issue: "jq: command not found"

**Symptom:**
```
bash: jq: command not found
```

**Solution:**
```bash
# Ubuntu/Debian
sudo apt-get install -y jq

# macOS
brew install jq
```

### Issue: "Database not found"

**Symptom:**
```
âš ï¸  Database not found: /tmp/claude_bridge_coordinator.db
```

**Cause:** Orchestrator hasn't created conversations yet.

**Solution:**
Wait for orchestrator to complete Step 3 (create 8 conversations). The database is created automatically when the first conversation is created.

### Issue: "Worker not sending heartbeats"

**Symptom:**
Watchdog logs show worker silent for >5 minutes.

**Diagnosis:**
```bash
# Check if keep-alive daemon is running
ps aux | grep keepalive-daemon

# Check daemon logs
tail -f /tmp/mcp-keepalive.log
```

**Solution:**
```bash
# Restart keep-alive daemon
pkill -f keepalive-daemon

# Re-extract credentials and restart
CONV_ID=$(jq -r '.conversation_id' credentials/s2-worker-${WORKER_ID}-credentials.json)
WORKER_TOKEN=$(jq -r '.worker_token' credentials/s2-worker-${WORKER_ID}-credentials.json)
scripts/s2-deployment/keepalive-daemon.sh "$CONV_ID" "$WORKER_TOKEN" &
```

---

## Files Reference

### Created in This Deployment

```
scripts/s2-deployment/
â”œâ”€â”€ README.md                      # Detailed setup instructions
â”œâ”€â”€ keepalive-daemon.sh            # Background polling (workers)
â”œâ”€â”€ keepalive-client.py            # Heartbeat updater (workers)
â”œâ”€â”€ watchdog-monitor.sh            # External monitoring (orchestrator)
â”œâ”€â”€ reassign-tasks.py              # Task reassignment (orchestrator)
â”œâ”€â”€ credential-sync-daemon.sh      # Auto-pull credentials (workers)
â”œâ”€â”€ check-messages.py              # Message checker (all)
â””â”€â”€ fs-watcher.sh                  # Filesystem watcher (optional)

credentials/
â”œâ”€â”€ README.md                      # Credential format docs
â”œâ”€â”€ s2-worker-template-credentials.json  # Template
â””â”€â”€ s2-worker-{1-8}-credentials.json     # Generated by orchestrator

docs/
â”œâ”€â”€ S2-MCP-BRIDGE-QUICKSTART.md          # Original quickstart
â”œâ”€â”€ S2-MCP-BRIDGE-TEST-PROTOCOL-V2.md    # Full test protocol (90 min)
â””â”€â”€ S2-MULTI-MACHINE-DEPLOYMENT.md       # This file
```

---

## Next Steps

1. âœ… **Deploy to all 9 machines** (follow steps above)
2. âœ… **Verify all workers sending heartbeats**
3. âœ… **Send test task to confirm communication**
4. ðŸ”„ **Run full Test Protocol V2** (see `S2-MCP-BRIDGE-TEST-PROTOCOL-V2.md`)
5. ðŸš€ **Begin production SÂ² coordination**

---

**Deployment Status:** Ready for Production âœ…
**Last Updated:** 2025-11-13
**Support:** See troubleshooting section above
