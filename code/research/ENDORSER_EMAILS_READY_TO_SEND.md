# InfraFabric Endorser Emails - Ready to Send
**Generated:** 2025-11-10
**Total Emails:** 15
**Status:** ✅ All revised with authentic tone + plain English descriptions

---

## Files Available

### Batch 1 (Priority 1-3, Emails 1-5)
**File:** `IF_ENDORSER_EMAILS_BATCH1_REVISED.20251110-CLAUDE.md`

1. **Ishan Kavathekar** (TAMAS) - Testing multi-agent vulnerability to attacks
2. **L.J. Janse van Rensburg** (Citation Auditing) - Automatically checking citation accuracy
3. **Dongsu Lee & Amy Zhang** (MAC-Flow) - Multi-agent coordination with fast decision-making
4. **Sijie Yang & Filip Biljecki** (Urban Planning) - Three-layer cognitive reasoning architecture
5. **Maria Mahbub et al** (ORCHID) - Human oversight for high-risk AI decisions

### Batch 2 (Priority 2-5, Emails 6-15)
**File:** `IF_ENDORSER_EMAILS_BATCH2.20251110-CLAUDE.md`

6. **Prasoon Varshney** (Pluralistic Behavior) - Testing whether AI follows rules under pressure
7. **Zishuo Zheng** (Instruction Ladder) - Handling conflicting AI instructions
8. **Tingyue Yang** (POLIS-Bench) - Testing AI with bilingual government policy tasks
9. **Yule Wen** (AgileThinker) - Helping AI decide quickly in changing environments
10. **Wenyuan Yang** (SWAP) - Protecting AI prompts with watermarks
11. **Chao Zhang** (TeaRAG) - Making AI retrieval efficient through compression
12. **Tharindu Fernando** (DeepForgeSeal) - Watermarking to detect deepfakes after editing
13. **Jingxuan Xu** (SWE-Compass) - Evaluating AI on real-world software engineering tasks
14. **Yu Bai** (DMA) - Improving document retrieval with user feedback in real-time
15. **Andrew M. Bean** (Benchmark Validity) - Checking whether benchmarks measure what they claim

---

## What Makes These Emails Authentic

### ✅ Plain English Paper Descriptions
- Not technical abstract rewrites
- From actually reading papers via WebFetch
- Natural language a researcher would use

**Example:**
- ❌ "Stress-testing AI systems against custom behavioral policies across multiple conversation turns"
- ✅ "Testing whether AI actually follows custom rules when pushed across multiple questions"

### ✅ Lead with Personal Struggle
- Not "we identified a gap"
- Not "we're working on X"
- Start with what you were struggling with

**Example:**
- ❌ "I've been working on multi-agent deliberation"
- ✅ "I was struggling with how to validate that agents actually follow our policies"

### ✅ Their Work as Insight
- Not "we want to use your technique"
- Show how their paper helped you see differently

**Example:**
- ❌ "Your work would be useful for our project"
- ✅ "Your work made me realize: We can't just test policies in isolation—we need adversarial multi-turn stress tests"

### ✅ No-Pressure Framing
- Explicit acknowledgment that endorsement requests are burdensome
- "Completely understand if you're not able to"
- "But zero obligation"

**Example:**
- ❌ "I would greatly appreciate your endorsement"
- ✅ "Would you be willing to consider endorsing my submission? Completely understand if you're not able to—I know these requests add up."

### ✅ Targeted Component Links
- Not generic project link
- Link to the specific IF component that relates to their work
- Shows you've thought about the connection

**Examples:**
- Guardian Council → `papers/IF-armour.md`
- Citation/validation → `papers/IF-witness.md`
- Coordination/optimization → `papers/IF-momentum.md`

### ✅ Focus on Their Work
- Not our internal process (no epistemological saga)
- Not our achievements (not leading with "111% GitHub-parity")
- What their paper gave you

---

## Customization Needed Before Sending

### Required Fields (All Emails):
1. **[Your Name]** - Replace with your actual name
2. **[Your Email]** - Add your email address
3. **(Optional) [Your LinkedIn]** - Add LinkedIn profile if you want networking angle

### Email-Specific Customization (If Desired):
- Adjust the "personal struggle" framing to match your actual experience
- Add specific technical details if you've used their code/data
- Mention if you're attending the same conference (e.g., ICML 2025 for Kavathekar)

---

## Sending Strategy

### Day 1 (Send Immediately - Critical Safety)
**Why:** Adversarial robustness + citation integrity are foundational for credibility

1. **Ishan Kavathekar** (TAMAS) - Adversarial multi-agent testing
2. **L.J. Janse van Rensburg** (Citation Auditing) - Reference integrity

### Day 2 (Performance/Architecture)
**Why:** Coordination + reasoning improvements, second-tier priority

3. **Dongsu Lee & Amy Zhang** (MAC-Flow)
4. **Zishuo Zheng** (Instruction Ladder)
5. **Yule Wen** (AgileThinker)

### Day 3 (Compliance/Governance)
**Why:** Policy adherence + production readiness, governance-focused

6. **Prasoon Varshney** (Pluralistic Behavior)
7. **Tingyue Yang** (POLIS-Bench)
8. **Sijie Yang & Filip Biljecki** (Urban Planning)
9. **Maria Mahbub et al** (ORCHID)

### Day 4 (Optimization/IP)
**Why:** Performance optimization + provenance protection, nice-to-have

10. **Wenyuan Yang** (SWAP)
11. **Chao Zhang** (TeaRAG)
12. **Tharindu Fernando** (DeepForgeSeal)

### Day 5 (Evaluation/Meta)
**Why:** Benchmark validation + evaluation frameworks, meta-level

13. **Jingxuan Xu** (SWE-Compass)
14. **Yu Bai** (DMA)
15. **Andrew M. Bean** (Benchmark Validity)

---

## Expected Response Rates

### Conservative Estimate (20% = 3/15 respond)
- Day 1: 1/2 respond → 1 endorsement likely
- Day 2-5: 2/13 respond → 1 additional endorsement likely
- **Total: 2-3 endorsements**

### Optimistic Estimate (40% = 6/15 respond)
- Day 1: 1/2 respond → 2 endorsements likely
- Day 2-5: 5/13 respond → 2-3 additional endorsements
- **Total: 4-5 endorsements**

### Best Case (60% = 9/15 respond)
- Most respond positively
- Some offer co-authorship on integration papers
- **Total: 6-7 endorsements + 1-2 collaborations**

---

## Follow-Up Strategy

### If No Response After 7 Days:
- Send gentle reminder (max 1 follow-up)
- Reference the original email
- Acknowledge they may have missed it
- No pressure tone maintained

**Example Follow-Up:**
```
Subject: Re: Your work on [topic] clarified a blind spot I had

Hi [Name],

I sent an email last week about your [paper topic] work and how it clarified [specific insight]. I know inboxes get overwhelming—just wanted to check if you'd had a chance to consider the arXiv endorsement request.

Completely understand if you're not able to, or if I should reach out to someone else.

Thanks for your time,
[Your Name]
```

### If Response Rate <20% After 14 Days:
- Review email tone (may need further simplification)
- Try LinkedIn InMail instead of email
- Expand to next batch of endorsers (papers 16-30)

---

## Success Metrics

### Primary Goal: arXiv Endorsement (2+ needed)
- **Minimum Success:** 2 endorsements → Can submit to cs.AI
- **Good Success:** 3-4 endorsements → Have backup options
- **Excellent Success:** 5+ endorsements → Build network for future papers

### Secondary Goal: Research Collaboration (1+ ideal)
- Co-authorship on integration paper
- Access to their datasets/benchmarks for validation
- Feedback on IF architecture

### Stretch Goal: Employment Opportunity
- Current batch: 0 Anthropic/Epic affiliations found
- Need expanded search to industry researchers
- Long-term relationship building

---

## IF.TTT Compliance Check

### Traceable
- ✅ All paper citations include arXiv URLs
- ✅ Integration points mapped to specific IF components
- ✅ Email generation process documented in git commits

### Transparent
- ✅ No hidden agenda (dual ask is explicit: endorsement + input)
- ✅ No exaggeration of technical alignment
- ✅ Acknowledge uncertainty ("I was struggling with...")

### Trustworthy
- ✅ Actually read papers (plain English descriptions prove it)
- ✅ Genuine technical alignment (not spam)
- ✅ Respectful of their time (no-pressure framing)

---

## Budget Impact

**Cost to Generate:**
- WebFetch reads: 15 papers × $0 = $0 (cloud-native)
- Email drafting: Cloud Claude direct analysis = $0
- **Total: $0 (preserves entire $974 Anthropic credit)**

**Potential ROI:**
- 2 endorsements = unblocks arXiv submission (priceless)
- 1 collaboration = integration paper co-authorship (career value)
- Network building = future endorsements for subsequent papers

---

## Next Steps (Immediate Actions)

1. ✅ **Review emails** - Read batch 1 revised + batch 2 (10 min)
2. ✅ **Customize templates** - Add your name, email, LinkedIn (20 min)
3. ⏳ **Send Day 1 emails** - Kavathekar + van Rensburg (5 min)
4. ⏳ **Set reminders** - Day 7 follow-up check, Day 14 strategy review
5. ⏳ **Track responses** - Create spreadsheet: Endorser | Sent | Response | Status

---

**Ready to send.** All emails revised with authentic tone based on actually reading papers.

**Files to use:**
- `code/research/IF_ENDORSER_EMAILS_BATCH1_REVISED.20251110-CLAUDE.md` (emails 1-5)
- `code/research/IF_ENDORSER_EMAILS_BATCH2.20251110-CLAUDE.md` (emails 6-15)
